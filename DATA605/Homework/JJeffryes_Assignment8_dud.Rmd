---
title: "DATA605 - Assignment 8"
author: "Jai Jeffryes"
date: "4/13/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(digits = 4)
```

## p. 303, ex. 11
A company buys 100 lightbulbs, each of which has an exponential lifetime of 1000 hours. What is the expected time for the first of these bulbs to burn out?

We know:

1. $n = 100$. (Problem definition)
2. The expected value for each bulb's lifetime is $\mu = 1000$. (Problem definition)
3. The distribution of lifetime is exponential. (Problem definition)
4. $M$ is the minimum of the distribution of the life of a single bulb, $x_j$. (Notation)
5. $E(M) = \frac{\mu}{n}$ (Given in problem 10)
6. $E(M) = \frac{1000}{100} = 10$ (1, 2, 5)

Answer: **The expective time for the first bulb to burn out is 10 hours.**

Note: This question was all about the answers hiding in plain sight. It will be worth returning to the section to learn more about exponential distributions.

## p. 303, ex. 14
Assume that $X1$ and $X2$ are independent random variables, each having an exponential density with parameter $\lambda$. Show that $Z = X_1 - X_2$ has density

$f_Z(z) = (1/2) \lambda e^{-\lambda|z|}$

### Visualization
This visualization is a dud because it's based on two values for $\lambda$. I don't know what the problem is really supposed to look like.

```{r}
lambda.1 <- 5
lambda.2 <- 2
x <- seq(from = 0, to = 2, by = 0.01)
y.1 <- lambda.1 * exp(-lambda.1 * x)
y.2 <- lambda.2 * exp(-lambda.2 * x)
y.3 <- y.1 - y.2
plot(x, y.1, type = "l", ylim = c(-4, 6),
	 ylab = "y",
	 main = "Exponential distributions: X1, X2, X1 - X2")
lines(x, y.2, col = "red")
lines(x, y.3, col = "blue")
legend(1.25, -1, legend=c("X1 = Exp(Lambda = 5)", "X2 = Exp(Lambda = 2)", "X1 - X2"),
       col=c("black", "red", "blue") , lty=c(1, 1, 1), cex=0.8)
```

If I plot the difference superimposed on the formula we're proving, the curves should be congruent. Let's try this experiment. Make one red, the other blue. The expected result is one line that is purple.

```{r}
lambda.1 <- 5
lambda.2 <- 2
x <- seq(from = 0, to = 2, by = 0.01)
y.1 <- lambda.1 * exp(-lambda.1 * x)
y.2 <- lambda.2 * exp(-lambda.2 * x)
y.3 <- y.1 - y.2
y.4 <- 
plot(x, y.1, type = "l", ylim = c(-4, 6),
	 ylab = "y",
	 main = "Exponential distributions: X1, X2, X1 - X2")
lines(x, y.2, col = "red")
lines(x, y.3, col = "blue")
legend(1.25, -1, legend=c("X1 = Exp(Lambda = 5)", "X2 = Exp(Lambda = 2)", "X1 - X2"),
       col=c("black", "red", "blue") , lty=c(1, 1, 1), cex=0.8)
```



## Extra study
- [L12.2 The Sum of Independent Discrete Random Variables](https://www.youtube.com/watch?v=zbu8KQx9bqM). This is the clearest video I found explaining convolution.
- [Convolution Theorem: Application & Examples](https://study.com/academy/lesson/convolution-theorem-application-examples.html). And this is the clearest article.

### Grinstead exercises
#### Ex. 7.1, p. 289
A die is rolled three times. Find the probability that the sum of the outcomes is

(a) greater than 9.
(b) an odd number.

I'll use a solution that counts the probabilities that I found on the internet and adapted in `R`. This helped me understand the summation when I added traces.

```{r}
# PMF
P <- function(x) {
	if (x %in% c(1:6)) {
		return(1 / 6)
	} else {
		return(0)
	}
}

# S(2). Include some traces.
S2 <- function(x, verbose = F) {
	total <- 0
	
	if (verbose) {
		print(paste0("x=", x))
	}
	for (k in 0:x) {
		total <- total + (P(k) * P(x - k))

		if (verbose) {
			print(paste0("k=", k, ", x-k=", x-k))
			print(paste0("    P(k)=", P(k), ", P(x-k)=", P(x-k)))
			print(paste0("    P(k)P(x-k)=", P(k) * P(x - k)))
			print(paste0("    total=", total))
		}
	}
	return(total)
}
    
S3 <- function(x, verbose = F) {
	total <- 0
	for (k in 0:x) {
		prob <- S2(k, verbose) * P(x - k)
		total <- total + prob
	}
	return(total)
}
```

Check out the PMF
```{r}
P(0)
P(3)
P(7)
```

Check out $P(S_2 = 2)$ and compare with p. 286.

```{r}
S2(2, verbose = T)
S2(3, verbose = T)
```
Check out $P(S_2 = 2)$ and compare with p. 287.

```{r}
S3(3, verbose = T)
```

##### Answer
- Part a
```{r}
total <- 0
for (i in 1:9) {
	total <- total + S3(i)
}

1 - total
```

- Part b
```{r}
total <- 0
for (i in 0:((3 * 6) - 1)) {
	if (i %% 2 == 1) {
		total <- total + S3(i)
	}
}

1 - total
```

#### Ex. 7.2, p. 289
The price of a stock on a given trading day changes according to the distribution

$p_X= (\frac{-1}{1/4} \frac{0}{1/2} \frac{1}{1/8} \frac{2}{1/8})$

Find the distribution for the change in stock price after two (independent) trading days.

The PMF represents the change after one day. Two days of changes is the convolution of the PMF with itself.

$p_X(x = 1) \cdot p_X(x = 1$

```{r}
# PMF
P <- function(x) {
	# if (x %in% c(1:6)) {
	# 	return(1 / 6)
	# } else {
	# 	return(0)
	# }
	dist <- c(01/4, 1/2, 1/8, 1/8)
	if (x %in% c(-1:2)) {
		return(dist[x + 1])
	} else {
		return(0)
	}
}

```

### Ex. 8.1.1
A fair coin is tossed 100 times. The expected number of heads is 50, and the standard deviation for the number of heads is (100 · 1/2 · 1/2)1/2 = 5. What does Chebyshev’s Inequality tell you about the probability that the number of heads that turn up deviates from the expected number 50 by three or more standard deviations (i.e., by at least 15)?

*Note*:

- What's given about standard deviation? We know the number of trials is 100 and the probability of the outcome heads. The formula for calculating standard deviation for the binomial distribution is $\sigma = \sqrt{npq}$.
- The question asks for *at least* 3 standard deviations. Use the tail form of the theorem.

$P(|X - \mu| \ge k \sigma) \ge \frac{1}{k^2}$

So, $P(|X - 50| \ge 3 \cdot 5) >= \frac{1}{9}$ = `r 1/9`.

The probability that the number of heads is at least 15 is, at most, `r 1/9`.

### Ex. 8.1.2
Write a program that uses the function binomial(n, p, x) to compute the exact probability that you estimated in Exercise 1. Compare the two results.

I wrote this function to visualize and calculate all the binomial intervals when I reviewed the binomial distribution section for DATA606. *Note:* there still might be some work to do on some of the boundaries.

```{r}
binom_summary <- function(successes, trials, prob,
                          main = NULL, xlab = "Successes", ylab = "P(X = x)") {
  # Annotate plot.
  param_str <- paste0(", n=", trials, ", p=", prob)
  main <- ifelse(is.null(main), paste0("Binomial distribution", param_str),
                 paste0(main, param_str))

  plot(0:trials, dbinom(0:trials, trials, prob = prob),
     xlab = xlab,
     ylab = ylab,
     type = "h",
     main = main)
  abline(v = successes, col = "red")
  
  mu <- trials * prob
  sigma <- sqrt(trials * prob * (1 - prob))
  is_normal_approx <- trials * prob >= 10 & trials * (1 - prob) >= 10

  print(paste0("mu: ", mu))
  print(paste0("sigma: ", sigma))
  
  print(paste0("Trials: ", trials))
  print(paste0("Successes: ", successes))

  prob.exact <- dbinom(successes, trials, prob)
  
  prob.cumulative.lt <- pbinom(successes - 1, trials, prob)
  # Normal approximation
  prob.cumulative.lt.approx <- pnorm(successes, mu, sigma)
  # Normal approximation with continuity correction.
  prob.cumulative.lt.approx.corr <- pnorm(successes - 0.5, mu, sigma)
  
  prob.cumulative.le <- pbinom(successes, trials, prob)
  # Normal approximation
  prob.cumulative.le.approx <- pnorm(successes, mu, sigma)
  # Normal approximation with continuity correction.
  prob.cumulative.le.approx.corr <- pnorm(successes + 0.5, mu, sigma)
  
  prob.cumulative.gt <- pbinom(successes, trials, prob, lower.tail = F)
  # Normal approximation
  prob.cumulative.gt.approx <- pnorm(successes, mu, sigma, lower.tail = F)
  # Normal approximation with continuity correction.
  prob.cumulative.gt.approx.corr <- pnorm(successes + 0.5, mu, sigma, lower.tail = F)

  prob.cumulative.ge <- pbinom(successes - 1, trials, prob, lower.tail = F)
  # Normal approximation
  prob.cumulative.ge.approx <- pnorm(successes, mu, sigma, lower.tail = F)
  # Normal approximation with continuity correction.
  prob.cumulative.ge.approx.corr <- pnorm(successes - 0.5, mu, sigma, lower.tail = F)

  print(paste0("Binomial probability. P(X=x): ", round(prob.exact, 4)))

  print(paste0("Cumulative probability. P(X<x): ", round(prob.cumulative.lt, 4)))
  if (is_normal_approx) {
    print(paste0("    Normal approximation. P(X<x): ",
                 round(prob.cumulative.lt.approx, 4)))
    print(paste0("    Normal approximation w/ continuity correction. P(<x): ",
                 round(prob.cumulative.lt.approx.corr, 4)))
  }
  
  print(paste0("Cumulative probability. P(X<=x): ", round(prob.cumulative.le, 4)))
  if (is_normal_approx) {
    print(paste0("    Normal approximation. P(X<=x): ",
                 round(prob.cumulative.le.approx, 4)))
    print(paste0("    Normal approximation w/ continuity correction. P(<=x): ",
                 round(prob.cumulative.le.approx.corr, 4)))
  }
  
  print(paste0("Cumulative probability. P(X>x): ", round(prob.cumulative.gt, 4)))
  if (is_normal_approx) {
    print(paste0("    Normal approximation. P(X>x): ",
                 round(prob.cumulative.gt.approx, 4)))
    print(paste0("    Normal approximation w/ continuity correction. P(X>x): ",
                 round(prob.cumulative.gt.approx.corr, 4)))
  }

    print(paste0("Cumulative probability. P(X>=x): ", round(prob.cumulative.ge, 4)))
  if (is_normal_approx) {
    print(paste0("    Normal approximation. P(X>=x): ",
                 round(prob.cumulative.ge.approx, 4)))
    print(paste0("    Normal approximation w/ continuity correction. P(X>=x): ",
                 round(prob.cumulative.ge.approx.corr, 4)))
  }
}
```

I ran it for this problem (incorrectly).

```{r}
binom_summary(15, 100, 0.5)
```

How great are pictures?! The successes I input, 15, is plotted with the red line. Instantly, I see my mistake. The successes I'm supposed to check don't amount to 15, they are the mean PLUS 3 standard deviations. Let's try again.

```{r}
binom_summary(50 + 15, 100, 0.5)
```
We want both tails, so twice that tail probability is `r 2 * 0.0018`,  rounded to 4 places. I'll calculate that again without rounding. The answer, `r 2 * pbinom(50 - 15, 100, 0.5)`. That is consistent with my answer for the previous problem which said that the probability *at most* is `r 1/9`. There can be a big margin around the results we calculate using Chebyshev's Inequality.